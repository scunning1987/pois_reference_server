#### Title : POIS Reference Server
#### Author : Scott Cunningham
####
####
## Parameters - User Input
Parameters:

  Placeholder:
    Description: placeholder
    Type: String
    Default: placeholder

## Resources
Resources:
  #################
  ## S3
  #################

  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName

  #################
  ## IAM & Permissions
  #################
  ## IAM Role
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName

  ## IAM Policy
  S3AccessPolicy:
    Type: AWS::IAM::Policy
    Properties:
      Roles:
        - !Ref LambdaRole
      PolicyName: !Sub ${AWS::StackName}-s3-access
      PolicyDocument:
        Statement:
          - Effect: Allow
            Action:
              - s3:*
            Resource:
              - !Sub arn:aws:s3:::${S3Bucket}/*
          - Effect: Allow
            Action:
              - s3:*
            Resource:
              - !Sub arn:aws:s3:::${S3Bucket}
          - Effect: Allow
            Action:
              - logs:CreateLogGroup
            Resource:
              - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*
          - Effect: Allow
            Action:
              - logs:CreateLogStream
              - logs:PutLogEvents
            Resource:
              - !Sub arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*
          - Effect: Allow
            Action:
              - dynamodb:*
            Resource:
              - !GetAtt POISDatabase.Arn

    DependsOn: S3Bucket

  LambdaInvokePermissionAPIGateway:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt MediaLiveControlFunctions.Arn
      Action: lambda:InvokeFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${ApiGateway}/*/*/dashboard-ctrl"
    DependsOn:
      - MediaLiveControlFunctions

  LambdaInvokePermissionAPIHandlerAPIGateway:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt MediaLiveControlConfig.Arn
      Action: lambda:InvokeFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${ApiGateway}/*/*/dashboard-cfg/*"
    DependsOn:
      - MediaLiveControlConfig

  LambdaInvokePermissionUIAccessProxy:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt UIAccessProxy.Arn
      Action: lambda:InvokeFunction
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${ApiGateway}/*/*/dashboard/*"
    DependsOn:
      - MediaLiveControlConfig

  LambdaInvokePermissionS3:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt ThumbnailS3PutRename.Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !GetAtt S3Bucket.Arn
    DependsOn:
      - S3Bucket
      - ThumbnailS3PutRename

  LambdaInvokedByEventBridge:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt MediaLiveScheduleCleanup.Arn
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt DailyScheduleCleanup.Arn
    DependsOn:
      - MediaLiveScheduleCleanup
      - DailyScheduleCleanup

  #####
  # DynamoDB
  #####
  POISDatabase:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub POIS-${AWS::StackName}
      BillingMode: PAY_PER_REQUEST
      KeySchema:
        - AttributeName: client_id
          KeyType: HASH
      AttributeDefinitions:
        - AttributeName: client_id
          AttributeType: S

  #################
  ## Custom Resource
  #################

  FileMover:
    Type: Custom::LambdaInvokerToMoveFiles
    Properties:
      ServiceToken: !GetAtt FileCopier.Arn
      Region: !Ref 'AWS::Region'
    DependsOn:
      - S3Bucket
      - LambdaRole

  #################
  ## Lambda
  #################

  ESAMProcessor:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda to handle ESAM requests and responses
      Role: !GetAtt LambdaRole.Arn
      Runtime: python3.8
      Handler: index.lambda_handler
      Timeout: 10
      MemorySize: 10240
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: !GetAtt FileMover.esam-processor
      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName
    DependsOn:
      - S3Bucket
      - LambdaRole
      - FileCopier
      - FileMover

  POISControl:
    Type: AWS::Lambda::Function
    Properties:
      Description: Function that handles API request to populate and configure POIS reference server
      Role: !GetAtt LambdaRole.Arn
      Runtime: python3.8
      Handler: index.lambda_handler
      Timeout: 10
      MemorySize: 10240
      Code:
        S3Bucket: !Ref S3Bucket
        S3Key: !GetAtt FileMover.pois-control
      Environment:
        Variables:
          BUCKET: !Ref S3Bucket
          CONFIG_KEY: !GetAtt FileMover.channel_map
          TEMPLATE_KEY: !GetAtt FileMover.channel_map_template
      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName
    DependsOn:
      - S3Bucket
      - LambdaRole
      - FileCopier
      - FileMover

  FileCopier:
    Type: AWS::Lambda::Function
    Properties:
      Description: Lambda function to copy solution files to destination bucket
      Role: !GetAtt LambdaRole.Arn
      Runtime: python3.8
      Handler: index.lambda_handler
      Timeout: 35
      MemorySize: 10240
      Code:
        ZipFile: |
          '''
          Copyright (c) 2021 Scott Cunningham

          Permission is hereby granted, free of charge, to any person obtaining a copy
          of this software and associated documentation files (the "Software"), to deal
          in the Software without restriction, including without limitation the rights
          to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
          copies of the Software, and to permit persons to whom the Software is
          furnished to do so, subject to the following conditions:

          The above copyright notice and this permission notice shall be included in all
          copies or substantial portions of the Software.

          THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
          IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
          FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
          AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
          LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
          OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
          SOFTWARE.

          Summary: This script is a custom resource to place the HTML pages and Lambda code into the destination bucket.

          Original Author: Scott Cunningham
          '''

          import json
          import logging
          import boto3
          import os
          import urllib3
          from urllib.parse import urlparse
          from zipfile import ZipFile
          import cfnresponse

          LOGGER = logging.getLogger()
          LOGGER.setLevel(logging.INFO)
          MANIFESTMODIFY="True"

          version = 2

          def lambda_handler(event, context):

              ## Log the incoming event
              LOGGER.info("Event : %s " % (event))

              ## Create Response Data Dictionary for the CloudFormationn response
              responseData = dict()

              ## Initialize S3 boto3 client
              s3 = boto3.client('s3')

              # Create urllib3 pool manager
              http = urllib3.PoolManager()

              # environment variables
              bucket = os.environ['BUCKET']
              apiendpoint = os.environ['APIENDPOINT']

              # Manifest File containning URL's on github
              cloudformation_manifest = "https://raw.githubusercontent.com/scunning1987/aws_projects/main/377/manifest.txt"

              # Get the manifest from GitHub
              get_response = http.request('GET', cloudformation_manifest)

              if get_response.status != 200:
                  # Exit the script with errors
                  responseData['Status'] = "Unable to get file from location : %s " % (file)
                  cfnresponse.send(event, context, "FAILED",responseData)
                  raise Exception("Unable to get file from location : %s " % (file))
              else:
                  # Continue and upload to S3
                  manifest_list = get_response.data.decode("utf-8").split("\n")

              # remove manifest.txt header line
              manifest_list.pop(0)

              LOGGER.info("Files to transfer to S3: %s " % (manifest_list))

              for file in manifest_list:

                  # Get the file from GitHub
                  if "http" in file:
                      get_response = http.request('GET', file)

                  if get_response.status != 200:
                      # Exit the script with errors
                      responseData['Status'] = "Unable to get file from location : %s " % (file)
                      cfnresponse.send(event, context, "FAILED",responseData)
                      raise Exception("Unable to get file from location : %s " % (file))
                  elif "http" in file:

                      # Continue and upload to S3

                      # url string to urllib object
                      file_url_formatted = urlparse(file)
                      file_url_path = file_url_formatted.path

                      # get path after github repo owner name - use this as the path to write to s3
                      path = '/'.join(file_url_path.split("/")[2:]).rsplit("/",1)[0]
                      s3_data = get_response.data


                      file_name = file.rsplit("/",1)[1]
                      file_base_name = os.path.splitext(file_name)[0]
                      s3_key = "%s/%s" % (path,file_name)

                      content_type = ""
                      if ".html" in file_name:
                          content_type = "text/html"
                      elif ".css" in file_name:
                          content_type = "text/css"
                      elif ".js" in file_name:
                          content_type = "text/javascript"
                      elif ".json" in file_name:
                          content_type = "application/json"
                      elif ".zip" in file_name: # this is the zip
                          content_type = "application/zip"
                          s3_key = path + file_name
                      elif ".py" in file_name:
                          # write python file to zip,
                          python_file = open("/tmp/"+file_name,"w")
                          python_file.write(get_response.data.decode("utf-8"))
                          python_file.close()

                          # Zip the file
                          LOGGER.info("Zipping the file : %s " % ("/tmp/"+file_name))
                          zipObj = ZipFile('/tmp/'+file_name.replace(".py",".zip"), 'w')
                          # Add file to the zip
                          zipObj.write('/tmp/'+file_name,"index.py")
                          # close the Zip File
                          zipObj.close()
                          LOGGER.info("Finished zipping file")

                          content_type = "application/zip"
                          s3_data = open("/tmp/"+file_name.replace(".py",".zip"), 'rb')
                          s3_key = s3_key.replace(".py",".zip")

                      # "RequestType": "Create"
                      if event['RequestType'] == "Create" or event['RequestType'] == "Update":
                          # Upload to S3
                          LOGGER.info("Now uploading %s to S3, Bucket: %s , path: %s" % (file_name,bucket,s3_key))
                          try:
                              s3_response = s3.put_object(Body=s3_data, Bucket=bucket, Key=s3_key,ContentType=content_type, CacheControl='no-cache')
                              LOGGER.info("Uploaded %s to S3, got response : %s " % (file_name,s3_response) )
                              responseData[file_base_name] = s3_key
                          except Exception as e:
                              LOGGER.error("Unable to upload %s to S3, got exception: %s" % (file_name,e))
                              responseData['Status'] = "Unable to upload %s to S3, got exception: %s" % (file_name,e)
                              cfnresponse.send(event, context, "FAILED",responseData)
                              raise Exception("Unable to upload %s to S3, got exception: %s" % (file_name,e))

                      else: # DELETE
                          try:
                              s3_response = s3.delete_object(Bucket=bucket,Key=s3_key)
                              LOGGER.info("Deleted %s from S3, got response : %s " % (file_name,s3_response) )
                          except Exception as e:
                              LOGGER.error("Unable to delete %s from S3, got exception: %s" % (file_name,e))
                              responseData['Status'] = "Unable to delete %s from S3, got exception: %s" % (file_name,e)
                              cfnresponse.send(event, context, "FAILED",responseData)
                  else:
                      LOGGER.info("Got line in manifest.txt that isn't a URL: %s " % (file))
              responseData['Status'] = "SUCCESS"
              cfnresponse.send(event, context, "SUCCESS",responseData)
              return responseData

      Environment:
        Variables:
          BUCKET: !Ref S3Bucket
          APIENDPOINT: !Sub https://${ApiGateway}.execute-api.${AWS::Region}.amazonaws.com/v1/dashboard-ctrl
      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName
        - Key: Version
          Value: 3
    DependsOn:
      - S3Bucket
      - LambdaRole
      - S3AccessPolicy

  ChannelMapUpdaterFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Channel configuration API handler
      Role: !GetAtt LambdaRole.Arn
      Runtime: python3.8
      Handler: index.lambda_handler
      Timeout: 10
      MemorySize: 10240
      Code:
        ZipFile: |
          '''
          Copyright (c) 2021 Scott Cunningham

          Permission is hereby granted, free of charge, to any person obtaining a copy
          of this software and associated documentation files (the "Software"), to deal
          in the Software without restriction, including without limitation the rights
          to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
          copies of the Software, and to permit persons to whom the Software is
          furnished to do so, subject to the following conditions:

          The above copyright notice and this permission notice shall be included in all
          copies or substantial portions of the Software.

          THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
          IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
          FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
          AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
          LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
          OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
          SOFTWARE.

          Summary: This script is a custom resource to place the HTML pages and Lambda code into the destination bucket.

          Original Author: Scott Cunningham
          '''

          import json
          import logging
          import boto3
          import os
          import urllib3
          from urllib.parse import urlparse
          from zipfile import ZipFile
          import cfnresponse

          LOGGER = logging.getLogger()
          LOGGER.setLevel(logging.INFO)
          MANIFESTMODIFY="True"

          def lambda_handler(event, context):

              ## Log the incoming event
              LOGGER.info("Event : %s " % (event))

              ## Create Response Data Dictionary for the CloudFormationn response
              responseData = dict()

              ## Initialize S3 boto3 client
              s3 = boto3.client('s3')

              # environment variables
              bucket = os.environ['BUCKET']
              s3_key = os.environ['CONFIG_KEY']
              api_ctrl = os.environ['APICTRL']

              LOGGER.info("Environment Variable : S3 Bucket : %s " % (bucket))
              LOGGER.info("Environment Variable : S3 Key : %s " % (s3_key))
              LOGGER.info("Environment Variable : API Ctrl : %s " % (api_ctrl))

              # "RequestType": "Create"
              if event['RequestType'] == "Create" or event['RequestType'] == "Update":
                  # Get Channel Map JSON from S3
                  LOGGER.info("Now getting Channel map json from S3")
                  try:
                      s3_response = s3.get_object(Bucket=bucket, Key=s3_key)
                      LOGGER.info("Got object from S3")
                  except Exception as e:
                      LOGGER.error("Unable to get object from S3, got exception: %s" % (e))
                      responseData['Status'] = "Unable to get object from S3, got exception: %s" % (e)
                      return cfnresponse.send(event, context, "FAILED",responseData)
                      raise Exception("Unable to get object from S3, got exception: %s" % (e))

                  # print s3 response
                  LOGGER.info("s3 object: %s " % (s3_response))

                  # Edit json with deployment information
                  s3_data = json.loads(s3_response['Body'].read())
                  s3_data['control_api_endpoint_url'] = api_ctrl
                  api_endpoint_parse = urlparse(api_ctrl)
                  s3_data['control_api_endpoint_host_header'] = api_endpoint_parse.netloc

                  content_type = "application/json"

                  # Put S3 Object back to bucket with changes
                  LOGGER.info("Now putting Channel map json back to S3")
                  try:
                      s3_response = s3.put_object(Body=json.dumps(s3_data), Bucket=bucket, Key=s3_key,ContentType=content_type, CacheControl='no-cache')
                      LOGGER.info("Put object to S3")
                      responseData['copy'] = s3_key
                  except Exception as e:
                      LOGGER.error("Unable to get object from S3, got exception: %s" % (e))
                      responseData['Status'] = "Unable to get object from S3, got exception: %s" % (e)
                      return cfnresponse.send(event, context, "FAILED",responseData)
                      raise Exception("Unable to get object from S3, got exception: %s" % (e))

              else: # DELETE
                  LOGGER.info("Nothing to do when DELETE stack command is used")


              responseData['Status'] = "SUCCESS"
              cfnresponse.send(event, context, "SUCCESS",responseData)
              return responseData

      Environment:
        Variables:
          BUCKET: !Ref S3Bucket
          CONFIG_KEY: !GetAtt FileMover.channel_map
          APICTRL: !Sub https://${ApiGateway}-${VPCEndpoint}.execute-api.${AWS::Region}.amazonaws.com/${ApiStage}/dashboard-ctrl
      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName
        - Key: Version
          Value: 2
    DependsOn:
      - ApiGateway


  S3BucketNotificationLambda:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Code:
        ZipFile: |

          from __future__ import print_function
          import json
          import boto3
          import cfnresponse

          SUCCESS = "SUCCESS"
          FAILED = "FAILED"

          print('Loading function')
          s3 = boto3.resource('s3')

          def lambda_handler(event, context):
              print("Received event: " + json.dumps(event, indent=2))
              responseData={}
              try:
                  if event['RequestType'] == 'Delete':
                      print("Request Type:",event['RequestType'])
                      Bucket=event['ResourceProperties']['Bucket']
                      delete_notification(Bucket)
                      print("Sending response to custom resource after Delete")
                  elif event['RequestType'] == 'Create' or event['RequestType'] == 'Update':
                      print("Request Type:",event['RequestType'])
                      LambdaArn=event['ResourceProperties']['LambdaArn']
                      Bucket=event['ResourceProperties']['Bucket']
                      Prefix=event['ResourceProperties']['Prefix'].rsplit("/",1)[0]
                      object = s3.Object(Bucket,Prefix+'/status.txt')
                      object.put(Body='status thumbnails directory')
                      add_notification(LambdaArn, Bucket, Prefix)
                      object.delete()
                      responseData={'Bucket':Bucket}
                      print("Sending response to custom resource")
                  responseStatus = 'SUCCESS'
              except Exception as e:
                  print('Failed to process:', e)
                  responseStatus = 'FAILED'
                  responseData = {'Failure': 'Something bad happened : %s .' % (e)}
              cfnresponse.send(event, context, responseStatus, responseData)

          def add_notification(LambdaArn, Bucket, Prefix):
              bucket_notification = s3.BucketNotification(Bucket)
              response = bucket_notification.put(
                NotificationConfiguration={
                  'LambdaFunctionConfigurations': [
                    {
                        'LambdaFunctionArn': LambdaArn,
                        'Events': [
                            's3:ObjectCreated:Put'
                        ],
                        'Filter':{'Key':{'FilterRules':[{'Name':'prefix','Value':Prefix}]}}
                    }
                  ]
                }
              )
              print("event notification put response: %s " % (response))
              print("Put request completed....")

          def delete_notification(Bucket):
              bucket_notification = s3.BucketNotification(Bucket)
              response = bucket_notification.put(
                  NotificationConfiguration={}
              )
              print("Delete request completed....")
      Runtime: python3.6
      Timeout: 50
    DependsOn:
      - S3Bucket
      - LambdaRole
      - S3AccessPolicy
      - ThumbnailS3PutRename
      - FileMover


    #################
    ## API Gateway
    #################

  ApiGateway:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: !Sub ${AWS::StackName}-emlctrl-api-handler
      Description: !Sub API Handler [${AWS::StackName}
      BinaryMediaTypes:
        - '*/*'
      EndpointConfiguration:
        Types:
          - PRIVATE
        VpcEndpointIds:
          - !Ref VPCEndpoint
      Policy: !Sub
        |-
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Principal": "*",
                    "Action": "execute-api:Invoke",
                    "Resource": "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*/*"
                },
                {
                    "Effect": "Deny",
                    "Principal": "*",
                    "Action": "execute-api:Invoke",
                    "Resource": "arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:*/*",
                    "Condition": {
                        "StringNotEquals": {
                            "aws:SourceVpc": "${VpcToDeployInto}"
                        }
                    }
                }
            ]
        }

      Tags:
        - Key: StackName
          Value: !Ref AWS::StackName

  ProxyPlusResourceCtrl:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !GetAtt ApiGateway.RootResourceId
      PathPart: 'dashboard-ctrl'
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  ProxyPlusResourceCfgRoot:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !GetAtt ApiGateway.RootResourceId
      PathPart: 'dashboard-cfg'
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  UIResourceRoot:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !GetAtt ApiGateway.RootResourceId
      PathPart: 'dashboard'
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  ProxyPlusResourceUIProxy:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !Ref UIResourceRoot
      PathPart: '{proxy+}'
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  ProxyPlusResourceCfgProxy:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !Ref ProxyPlusResourceCfgRoot
      PathPart: '{proxy+}'
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  AnyMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      AuthorizationType: NONE
      HttpMethod: ANY
      Integration:
        #Credentials: !GetAtt ApiGatewayRole.Arn
        IntegrationHttpMethod: POST
        IntegrationResponses:
          - StatusCode: '200'
        Type: AWS_PROXY
        Uri: !Join
          - ''
          - - 'arn:aws:apigateway:'
            - !Ref 'AWS::Region'
            - :lambda:path/2015-03-31/functions/
            - !GetAtt MediaLiveControlConfig.Arn
            - /invocations
      MethodResponses:
        - StatusCode: 200
          ResponseModels:
            application/json: !Ref EmptyApiModel
      ResourceId: !Ref ProxyPlusResourceCfgProxy
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  UIMethods:
    Type: AWS::ApiGateway::Method
    Properties:
      AuthorizationType: NONE
      HttpMethod: ANY
      Integration:
        #Credentials: !GetAtt ApiGatewayRole.Arn
        IntegrationHttpMethod: POST
        IntegrationResponses:
          - StatusCode: '200'
        Type: AWS_PROXY
        Uri: !Join
          - ''
          - - 'arn:aws:apigateway:'
            - !Ref 'AWS::Region'
            - :lambda:path/2015-03-31/functions/
            - !GetAtt UIAccessProxy.Arn
            - /invocations
      ResourceId: !Ref ProxyPlusResourceUIProxy
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  GetMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      AuthorizationType: NONE
      HttpMethod: GET
      Integration:
        #Credentials: !GetAtt ApiGatewayRole.Arn
        IntegrationHttpMethod: POST
        IntegrationResponses:
          - StatusCode: '200'
        Type: AWS_PROXY
        Uri: !Join
          - ''
          - - 'arn:aws:apigateway:'
            - !Ref 'AWS::Region'
            - :lambda:path/2015-03-31/functions/
            - !GetAtt MediaLiveControlFunctions.Arn
            - /invocations
      MethodResponses:
        - StatusCode: 200
          ResponseModels:
            application/json: !Ref EmptyApiModel
      ResourceId: !Ref ProxyPlusResourceCtrl
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  PutMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      AuthorizationType: NONE
      HttpMethod: PUT
      Integration:
        #Credentials: !GetAtt ApiGatewayRole.Arn
        IntegrationHttpMethod: POST
        IntegrationResponses:
          - StatusCode: '200'
        Type: AWS_PROXY
        Uri: !Join
          - ''
          - - 'arn:aws:apigateway:'
            - !Ref 'AWS::Region'
            - :lambda:path/2015-03-31/functions/
            - !GetAtt MediaLiveControlFunctions.Arn
            - /invocations
      MethodResponses:
        - StatusCode: 200
          ResponseModels:
            application/json: !Ref EmptyApiModel
      ResourceId: !Ref ProxyPlusResourceCtrl
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  OptionsForCors:
    Type: AWS::ApiGateway::Method
    Properties:
      AuthorizationType: NONE
      HttpMethod: OPTIONS
      Integration:
        IntegrationResponses:
          - StatusCode: 200
            ResponseParameters:
              method.response.header.Access-Control-Allow-Headers: "'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token'"
              method.response.header.Access-Control-Allow-Methods: "'GET,PUT,OPTIONS'"
              method.response.header.Access-Control-Allow-Origin: "'*'"
            ResponseTemplates:
              application/json: ''
        PassthroughBehavior: WHEN_NO_MATCH
        RequestTemplates:
          application/json: '{"statusCode": 200}'
        Type: MOCK
      MethodResponses:
        - StatusCode: 200
          ResponseModels:
            application/json: 'Empty'
          ResponseParameters:
            method.response.header.Access-Control-Allow-Headers: false
            method.response.header.Access-Control-Allow-Methods: false
            method.response.header.Access-Control-Allow-Origin: true
      ResourceId: !Ref ProxyPlusResourceCtrl
      RestApiId: !Ref ApiGateway
    DependsOn:
      - ApiGateway

  EmptyApiModel:
    Type: AWS::ApiGateway::Model
    Properties:
      ContentType: application/json
      Description: This is a default empty schema model
      RestApiId: !Ref ApiGateway
      Schema: {
        "$schema": "http://json-schema.org/draft-04/schema#",
        "title": "Empty Schema",
        "type": "object"
      }

  Deployment:
    Type: AWS::ApiGateway::Deployment
    Properties:
      Description: Production Deployment of Api Endpoint
      RestApiId: !Ref ApiGateway
    DependsOn:
      - AnyMethod
      - GetMethod
      - PutMethod
      - ApiGateway

  ApiStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      DeploymentId: !Ref Deployment
      RestApiId: !Ref ApiGateway
      StageName: v1
    DependsOn:
      - ApiGateway

    #################
    ## VPC
    #################

  VPCEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      ServiceName: !Sub com.amazonaws.${AWS::Region}.execute-api
      PrivateDnsEnabled: true
      SubnetIds: !Ref SubnetToDeployApiInterface
      VpcId: !Ref VpcToDeployInto
      VpcEndpointType : Interface
      SecurityGroupIds: !Ref InterfaceEndpointSecurityGroups

    #################
    ## EventBridge
    #################

  DailyScheduleCleanup:
    Type: AWS::Events::Rule
    Properties:
      Description: This Event Rule will run at midnight every night to cleanup schedule events in MediaLive Channels that are part of the workflow
      Name: MediaLiveScheduleCleanup
      RoleArn: !GetAtt LambdaRole.Arn
      State: ENABLED
      ScheduleExpression: cron(0 0 * * ? *)
      Targets:
        - Arn: !GetAtt MediaLiveScheduleCleanup.Arn
          Id: LambdaForMediaLiveCleanup
          RetryPolicy:
            MaximumRetryAttempts: 4
            MaximumEventAgeInSeconds: 600
    DependsOn:
      - MediaLiveScheduleCleanup
      - LambdaRole

#################################
# Outputs
#################################

Outputs:
  DashboardUrl:
    Description: Dashboard URL for MediaLive control
    #Value: !Sub https://${S3Bucket}.s3.${AWS::Region}.amazonaws.com/${FileMover.ui}
    #Value: !Sub https://${CloudFrontDistribution.DomainName}/${FileMover.ui}
    Value: !Sub https://${ApiGateway}-${VPCEndpoint}.execute-api.${AWS::Region}.amazonaws.com/${ApiStage}/dashboard/${S3Bucket}/${FileMover.ui}

  APIEndpointURLGetSample:
    Description: API Endpoint for getting sample configuration, method - GET
    #Value: !Sub https://${ApiGateway}.execute-api.${AWS::Region}.amazonaws.com/${ApiStage}/${ProxyPlusResourceCfgRoot.PathPart}/template
    Value: !Join
      - ''
      - - 'https://'
        - !Ref ApiGateway
        - '-'
        - !Ref VPCEndpoint
        - '.execute-api.'
        - !Ref 'AWS::Region'
        - '.amazonaws.com/'
        - !Ref ApiStage
        - '/dashboard-cfg/template'

  APIEndpointURLGetConfig:
    Description: API Endpoint for getting existing configuration, method - GET
    Value: !Sub https://${ApiGateway}-${VPCEndpoint}.execute-api.${AWS::Region}.amazonaws.com/${ApiStage}/dashboard-cfg/existing

  APIEndpointURLPutConfig:
    Description: API Endpoint for updating existing configuration, method - PUT
    Value: !Sub https://${ApiGateway}-${VPCEndpoint}.execute-api.${AWS::Region}.amazonaws.com/${ApiStage}/dashboard-cfg/update

  ProxyThumbnailToS3PathTemplate:
    Description: For the multiviewer, each proxy MediaLive channel should output a thumbnail to this location. Take note of the jpg location as it will be required in the channel_map.json. DO NOT PUT AN EXTENSION IN THE NAME IN MEDIALIVE OR A NAME MODIFIER
    Value: !Sub s3://${S3Bucket}/status_thumbnails/[thumbnail-name]

  #APIEndpointURLCTRL:
  #  Description: API Endpoint to control the MediaLive channels. This URL is needed when updating the config
  #  Value: !Sub https://${ApiGateway}.execute-api.${AWS::Region}.amazonaws.com/${ApiStage}/dashboard-ctrl

